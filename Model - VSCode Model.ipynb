{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directories\n",
    "import random\n",
    "import shutil\n",
    "##\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "## Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "# Keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "## from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "# Image processing\n",
    "from keras.preprocessing import image\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Time\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%run Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move photos to Train, Validation and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define directorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main directory\n",
    "main_directory = \"Esquerda\\Images_Folder\"\n",
    "\n",
    "# Define the subdirectories\n",
    "## Letters of the alphabet\n",
    "letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "## Data set is divided in 3: Train, Validation, and Test\n",
    "datasets = ['Train', 'Test', 'Validation']\n",
    "## Dictionary for the predictions given they return values from 0 to 25\n",
    "mapping_dict = {\n",
    "    0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J',\n",
    "    10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S',\n",
    "    19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70% - Train, 15% - Validation, 15% - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage splits\n",
    "train_percentage = 0.7\n",
    "validation_percentage = 0.15\n",
    "test_percentage = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for letter in letters:\n",
    "        # Create destination directories if they don't exist\n",
    "        destination_dir = os.path.join(main_directory, dataset, letter)\n",
    "        os.makedirs(destination_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ LETTER A ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER B ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER C ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER D ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER E ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER F ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER G ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER H ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER I ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER J ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER K ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER L ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER M ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER N ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER O ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER P ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER Q ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER R ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER S ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER T ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER U ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER V ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER W ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER X ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER Y ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n",
      "------ LETTER Z ------\n",
      "Total Number: 0 Train count: 0 Validation count: 0 Test count: 0\n"
     ]
    }
   ],
   "source": [
    "test_letter_count = []\n",
    "total_letter_count = 0\n",
    "\n",
    "loop = 0\n",
    "for dataset in datasets:\n",
    "    loop += 1\n",
    "    for letter in letters:\n",
    "\n",
    "        # Source directory for the letters\n",
    "        source_dir = os.path.join(main_directory, letter)\n",
    "\n",
    "        # List files in the source directory\n",
    "        files = os.listdir(source_dir)\n",
    "        num_files = len(files)\n",
    "\n",
    "        # Calculate the number of files for each destination\n",
    "        train_count = int(num_files * train_percentage)\n",
    "        test_count = int(num_files * test_percentage)\n",
    "        validation_count = num_files - train_count - test_count\n",
    "        total_letter_count += num_files\n",
    "\n",
    "        # Shuffle the files\n",
    "        random.shuffle(files)\n",
    "\n",
    "        # Move files to their respective destinations based on percentages\n",
    "        for i, file in enumerate(files):\n",
    "            if i < train_count:\n",
    "                dest = os.path.join(main_directory, 'Train', letter, file) \n",
    "            elif i < train_count + validation_count:\n",
    "                dest = os.path.join(main_directory, 'Validation', letter, file)\n",
    "            else:\n",
    "                dest = os.path.join(main_directory, 'Test', letter, file)\n",
    "            shutil.move(os.path.join(source_dir, file), dest)\n",
    "\n",
    "        if loop == 1:\n",
    "            test_letter_count.append(test_count)\n",
    "            print('------ LETTER', letter, '------')\n",
    "            print('Total Number:',num_files, 'Train count:', train_count, 'Validation count:', validation_count, 'Test count:', test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_letter_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2, #lay down ESCREVER \n",
    "    zoom_range=0.2,\n",
    "    fill_mode='nearest', # Strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
    "    rescale=1./255 # Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our models to process (given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "    #horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=r\"./Images_Folder/Train\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    #color_mode='grayscale',\n",
    "    class_mode=\"categorical\",\n",
    "    #classes=letters,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images_generator(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, labels_traind = next(train_generator)  # Load a batch of images and labels\n",
    "plt.imshow(image.array_to_img(images_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    directory=r\"./Images_Folder/Validation\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\",\n",
    "    #classes=letters,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = len(letters)\n",
    "num_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing CNN layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(num_of_classes, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since our data set is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_spets = len(train_generator)\n",
    "num_val_spets = len(validation_generator)\n",
    "\n",
    "print('Number of Train steps:',  num_train_spets)\n",
    "print('Number of Validation steps:',  num_val_spets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use fit and not fit_generator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "## Adicionar early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = num_train_spets,\n",
    "    epochs=100,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=num_val_spets, \n",
    "    callbacks=[es]\n",
    ")\n",
    "\n",
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=r\"./Images_Folder/Test\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode=\"categorical\",\n",
    "    #classes=letters,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_pred, labels_pred = next(test_generator)  # Load a batch of images and labels\n",
    "plt.imshow(image.array_to_img(images_pred[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_generator)\n",
    "results = np.argmax(pred,axis=-1)\n",
    "mapped_results = [mapping_dict.get(item) for item in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = []\n",
    "for i in range(len(letters)):\n",
    "    for j in range(test_letter_count[i]):\n",
    "        expected.append(letters[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(expected, mapped_results, target_names=letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(true, preds):\n",
    "   conf_matx = confusion_matrix(true, preds)\n",
    "   sns.heatmap(\n",
    "      conf_matx, \n",
    "      annot=True, \n",
    "      annot_kws={\"size\": 12},\n",
    "      fmt='g', \n",
    "      cbar=False, \n",
    "      cmap=\"viridis\",\n",
    "      xticklabels=letters,\n",
    "      yticklabels=letters\n",
    "\n",
    "   )\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confusion_matrix(expected, mapped_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_prediction=np.array([expected,mapped_results]).T\n",
    "df_prediction=pd.DataFrame(array_prediction, columns=['Expected','Predicted'])\n",
    "df_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save log\n",
    "May not save the model due to it's size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May not save the model due to it's size\n",
    "save_model_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in letters:\n",
    "    for dataset in datasets:\n",
    "        source_dir = os.path.join(main_directory, dataset, letter)\n",
    "        files = os.listdir(source_dir)\n",
    "        for file in files:\n",
    "            dest = os.path.join(main_directory, letter, file)\n",
    "            shutil.move(os.path.join(source_dir, file), dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
